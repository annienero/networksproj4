We began by just trying to get a basic HTTP interface working that would let us send any kind of request with whatever body and headers we liked, and return the server's response. Once we had this, we made the main logic that actually crawled the pages. The rest of the work after this was working out bugs in our code and implementing more robust HTTP interface. Our main obstacles were dealing with unforeseen complications in how the server might respond to our requests, such as 4xx status codes 500 status codes, and chunking. We tested our code by just running it on the server, and debugged with a lot of print statements.
Initially we also ran into some trouble figuring out how authentication worked. To do this we reverse engineered what the requests should look like by watching them happen in our browser.
